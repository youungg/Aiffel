사조새 발표 시작하겠습니다
 
실험 내용은 pascal voc 2012 dataset을 각 모델에 대해 학습을 진행하였습니다.
----
목차는 다음과 같이 ~~~~로 구성 되어 있습니다.
----
구성원은 김한나, 이재영, 전휘호 이고, 사용 data는 pascal voc 2012 이며, 사용 모델은 Deeplab v3 +, FCN, backbone을 활용한 UNET with REsnet50 입니다
각 모델은 deeplab, FCN, unet 순서로 김한나, 이재영, 전휘호가 각각 작성하였습니다.
---
우선적으로, 데이터 전처리는 사진과 같이, 중복되는 색상값들을 제거한 후에 키값을 부여하였고
모델에 노이즈를 추가하여 성능을 향상시키기위해 데이터의 색상값과 밝기, 대비, 회전을 주는 어그멘테이션을 진행하였습니다.
그리고 데이터를  train dataset이 2039개 validation이 582개, test가 291개 로 분할 하여 진행하였습니다.

발표는 각 모델별  모델설명, 작성코드, 결과 순서로 진행하겠습니다.

-----------------------
발표할 처음 모델은 Deeplab V3+입니다. 
DeepLab V3+는 세그멘테이션 분야에서 주목받는 모델 중 하나로 인코더와 디코더 구조를 가지고 있어, 물체의 선명한 경계선을 효과적으로 포착할 수 있는 구조적 특성을 가지고 있습니다. 
(이를 위해 DeepLab V3+는 인코더와 디코더 구조를 결합하여 물체의 형태를 보다 정확하게 추출하는 데 중점을 둡니다.)

또한, DeepLab V3+는 Atrous Convolution이라는 구조를 사용하는데 ( ```이는 일반적인 컨볼루션 레이어 사이에 공간을 두어 마치 구멍이 뚫린 듯한 구조를 형성합니다. ```)
이러한 접근 방식은 같은 연산량으로 더 넓은 범위의 이미지 특징을 포착할 수 있게 하며, 따라서 더 크고 복잡한 패턴을 인식하는 데 유리합니다.

(
이러한 두 가지 구조적 특성의 조합은 DeepLab V3+가 일반적인 인코더-디코더 구조의 한계를 넘어서게 합니다.
 Atrous Convolution을 통해 인코더에서 추출된 특징맵의 해상도를 임의로 조절할 수 있게 되어, 세부적인 이미지 세그멘테이션에서 뛰어난 성능을 발휘할 수 있습니다
.)
------------------------

DeepLab V3+ 모델에서 ASPP(Atrous Spatial Pyramid Pooling) 구조는 이미지 세그멘테이션의 핵심 요소 중 하나입니다. 이 구조의 주요 특징은 ‘rate’, 즉 atrous convolution의 확장비율(dilation rate)에 있고,
이 비율은 atrous convolution이 이미지의 얼마나 넓은 영역을 고려할 것인지 결정하는 (중요한) 파라미터입니다.

이 모델의 인코더 부분을 통과한 후 생성되는 특징맵은 원본 이미지의 해상도보다 16배 작아집니다. 이것은 Output Stride가 16으로 설정되어 있기 때문입니다. 
따라서, 특징맵의 디테일을 효과적으로 회복하고, 이미지의 정밀한 세그멘테이션을 달성하기 위해 디코딩 과정에서 Low-Level Feature를 결합하여 사용합니다.

Output Stride를 16으로 설정한 것은 다양한 실험을 통해 얻은 결과입니다. 이러한 설정은 세그멘테이션 작업에서 처리 속도와 정확도 사이의 최적의 균형점을 제공합니다.

 즉, 빠른 처리 속도를 유지하면서도 높은 정확도의 세그멘테이션 결과를 얻을 수 있습니다. 이는 DeepLab V3+가 효율적이면서도 정밀한 세그멘테이션을 수행할 수 있는 이유 중 하나입니다.

-----------------------

Deeplab의 경우 오픈소스 리소스를 최대한 활용하여 진행되었습니다. 
오픈소스 소프트웨어의 활용은 개발 과정의 효율성을 높이고, 이미 검증된 기술을 바탕으로 안정적인 성능을 기대할 수 있게 합니다. 
우리는 이러한 오픈소스 솔루션이 제공하는 퍼포먼스가 우리가 사용하는 데이터셋에도 동일하게 적용될 것으로 기대하였습니다

----------------------------------

여긴 일단 킵

사용한 오픈소스는 모델의 전체적인 output stride를 줄이는 방법을 채택했습니다. 
이러한 접근법은 다양한 데이터셋에서 모델을 사전 훈련(pretraining)할 때 모델의 성능을 향상시키는 데 중요한 역할을 했습니다. 
Output stride를 줄임으로써, 특징맵의 공간 해상도를 높이고, 더욱 세밀한 이미지 세그멘테이션을 달성할 수 있었습니다.
뿐만 아니라, 이 연구는 변형된 Xception 아키텍처를 기반으로 합니다.
 Xception 아키텍처는 이미 그 자체로 강력한 성능을 보이는 구조이지만, JFT 데이터셋에서의 사전 훈련을 통해 이 모델을 더욱 강화했습니다. 
JFT 데이터셋은 매우 광범위하고 다양한 이미지를 포함하고 있어, 모델이 더욱 다양한 특징을 학습할 수 있게 합니다.


-------------------------

본 연구 과정에서 저희 팀은 오픈소스 환경과 우리 실험 환경 간의 차이, 특히 GPU의 수와 같은 요소들로 인해 몇 가지 오류에 직면하게 되었습니다. 
이러한 문제를 해결하기 위해, 저희는 배치 정규화 설정을 저희 데이터셋과 학습 환경에 맞게 조정하는 중요한 작업을 수행했습니다.

특히, DeepLab 모델을 초기화하는 과정에서 저희는 여러 핵심 parameter들을 세심하게 조정했습니다. 
이에는 모델의 backbone, 즉 인코더의 아키텍처, output stride, 그리고 클래스 수 등이 포함되었습니다. 

((((이 과정에서 많은 시간이 소요되었습니다.)))))


추후에 위 오류에 대하여 수정한 후 해당 부분에 대하여 추가 실험을 진행할 예정입니다. 

------------------

다음으로는 FNC에 대하여 발표하겠습니다. 
우선 FNC는 Fully convolutional network의 약자로 모든 블락들이 컨볼루션으로 이루어져 있음을 의미합니다.
 
기존 Image classification 모델들은 기본적으로 내부 구조와 관계없이 
모델의 근본적인 목표를 위해 출력층이 Fully-connected layer(이하 fc-layer) 로 구성되어있지만,

 Semantic Segmentation 관점에서는 feature map 의 spatial information(위치 정보)이 사라진다는 한계점을 지닙니다.
(fc-layer로 넘어가는 과정에서 feature map이 flatten 되면 위치에 대한 정보가 손실된다고 간주하기 때문에 spatial coordinate을 잃어버린다고 표현)

이를 보안하기 위해, FC layer를 Conv-layer로 대체하였습니다.

(좌측 그림은 논문에 나온 그림을 가져온 것이며, 앞서 설명한 내용을 그림으로 표현 한 것 입니다.)

(
'''
FCN은 Deeplab v3+ 와 같이 Encoder + Decoder 구조로 구현되어있으며 
Encoder 부분에서 ImageNet으로 학습한 pre-trained CNN model 사용합니다.

그리고, skip architecture 을 이용하여 
(coarse 한 부분인) deep layer 의 semantic information 와 (fine 부분인 )   shallow layer 의 appearance information 을 혼합하여 segmentation 에 사용합니다.

FNC는 VGG19를 backbone으로 한 CNN 구조를 지니며
인풋 이미지를 컨벌루션 연산을 통해 feature를 추출하고


1x1 Filter를 이용해 Spatial한 정보를 유지한 상태로 heat map(M x N x class)을 생성합니다.
그후,
Skip architecture를 통하여 Heat map을 Pooling Layer의 결과들과 combine하고
Upsampling (Deconvolution) 으로Combined된 Heap map을 
Input 이미지와 동일한 사이즈(W x H)로 복원합니다.

'''
)

---------------------

코드를 확인하면 block들은 vgg19 모델을 간단히 구현한 것 이며
 fc에서 fully connected layer를 convolution layer로 구현 한것 입니다.
convolution을 진행할때, padding = 'valid'로 작성되어 1x1x4096 및 1x1xclasses 의 형태로 구현되어있으며,
이를 업샘플링한 후, crop시켜 인풋 이미지 사이즈와 같은 형태로 만들어 줍니다.

------
이 코드를 구현한 결과는 다음과 같습니다. 

loss와 accuracy는 좌측 그림과 같이 나타나며, predict mask는 우측 그림과 같이 나타납니다.
결과를 통하여 predicted mask와 ground truth는 유사하지않다는 것을 알 수 있습니다..
((recall(실제 true 중에서, 모델이 true 라고 예측한것) 값이 낮게 나타남을 알수있다.))


---------------------
16stride의 앞구조는 32stride와 같고, 이후 block4를 convolution 진행하고 업샘플링 된 부분과 add한 후
업샘플링을 다시 진행하는 구조이다.

-----

이 결과 이전 보다는 유의미한 결과를 얻었음을 알수 있지만 작은 형태에 대해서는
결과가 좋지 않음을 알 수 있다.

-----

8s 구조의 경우,  block4 이외에 block3도 16s 와 같이 convolution, 업샘플링, add, upsampling을 진행하는 구조입니다.
------
그 결과 16s 에 비해 정성적인 평가로 성능이 더 나아졌음을 알 수 있습니다.

----

이후에 추가적인 실험을 진행하였고,
fc 에 대하여 padding 이 valid가 아닌 
padding이 same일 경우에 대하여 비교하였습니다
----

그 결과 32s 부터 predict mask 의 형태가 나타났습니다.

---
  16s 에 대한 코드이고

-----
이역시도 결과가 valid에 비하여 나쁘지 않다고 판단할 수 있습니다.

----

마지막으로 8s 에 대한 코드이며
----

오차는 존재하지만 ground truth와 유사하게 나타 났음을 알 수있습니다.

---

한눈에 비교하면 다음과 같습니다. 

이 결과들를 보완하기위한 batch size변경, dropout, batchnormaliztion 추가 등 실험을 진행할 예정입니다.

valid 의 경우 마지막 업샘플링시 output 형태를 맞추기위하여 기존 논문에서 구현된 stride를 2배하여 사용하였고 
kernel_size 역시 custom된 형태입니다.

이를 기본 논문에 구현된 형태로 stride와 kernel size로 작성되었을 때, output 형태가 input형태와 같게끔 하는 코드를 추가적으로 작성해 볼 계획입니다.   

-----------


마지막 모델은 backbone으로 Resnet50을 구현한 U-Net 모델입니다

 U-Net은 주로 이미지 세그멘테이션 작업에서활용되는데,
이미지 세그멘테이션 작업에서는 
이미지의 복잡한 특징을 정확하게 파악하는 것이 중요하기에 
ResNet50의 강력한 특징 추출 능력과 깊은 네트워크 구조를 백본으로 활용했습니다.

--------
저희가 이번에 구현한 레이어는 해당 사진처럼 구성이 되었습니다.

conv_block에서 컨볼루션 레이어과 배치 정규화, 그리고 활성화 함수를 순차적으로 
적용하고 선택적 드롭아웃을 추가로 적용 시켰습니다.
이 과정을 두번 반복하였습니다.

Encoder Block에서는 위에서 정의한 conv_block을 사용하여 특징을 추출한 후 
다운샘플링을 진행하였습니다.

Decoder Block에서는 업샘플링을 위해 컨볼루션을 사용하였고,
업샘플링된 맵과 인코더를 연결하여 skip connection을 형성했습니다.
그 후 연결된 맵에 다시 컨볼루션을 적용하였습니다.

마지막으로 build_model부분에서는 ResNet50 백본 모델을  include_top=False를 활용하여 네트워크의 분류 층을 제외한 상태로 불러와서 
이미지의 특징을 추출하고
인코더와 디코더 사이의 정보 흐름을 원활하게 하였습니다

세그멘테이션 작업의 효율성과 정확도를 높이기 위해 Bridge층을 구현하였고,
ResNet50 백본의 첫 ReLU 활성화 함수를 진행한 후 출력을 가져와서
특징 맵의 크기를 조정했습니다. 

업샘플링과 skip connection을 통해 맵을 확장한 후 이전 층의 정보와 결합했습니다.
마지막 최종 출력을 위해 1x1 컨볼루션 레이어를 사용했습니다.
------------------------
결과 비교 화면의 경우는 train_dataset에 대한 예측은 어느정도 일치하게 나왔지만
test_dataset에서는 사진에 배경이 단순할경우만 유사하게 예측하고 
여러가지 물체가 복합적으로 섞인 사진에서는 예측률이 떨어지는걸 볼 수 있습니다.
마지막으로, validation_dataset에서는 유사하게 예측을 하였지만 색상값이 
조금 다른걸 볼 수 있습니다.
-----------------
해당 결과를 토대로 원본 이미지와 유사하게 변환하려고 시도 하였으나, RGB값에서 오류가
발생해서 색상을 정확하게 재연하지 못했기에 이부분은 추후 보완해보려고 합니다. 
