{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e020134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # 메모리 사용 제한을 위한 설정\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    # 특정 GPU만 사용하도록 설정\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작 후에는 GPU 설정을 변경할 수 없으므로\n",
    "    # 런타임 오류 발생 시 예외 처리가 필요함\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663bd856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3768c783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a387136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2ce589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39d5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6add76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1205d9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I write, erase, rewrite\n",
      "Vectorized Text: [7 3 2 5]\n",
      "\n",
      "Original Text: Erase again, and then\n",
      "Vectorized Text: [ 2 10  9  4]\n",
      "\n",
      "Original Text: A poppy blooms.\n",
      "Vectorized Text: [11  6  8  0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 텍스트를 확인할 입력 데이터\n",
    "input_texts = [\"I write, erase, rewrite\", \"Erase again, and then\", \"A poppy blooms.\"]\n",
    "\n",
    "# 텍스트를 vectorization\n",
    "vectorized_texts = text_vectorization(input_texts)\n",
    "\n",
    "# 결과 출력\n",
    "for input_text, vectorized_text in zip(input_texts, vectorized_texts.numpy()):\n",
    "    print(f\"Original Text: {input_text}\")\n",
    "    print(f\"Vectorized Text: {vectorized_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48e9617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Texts:\n",
      "I write, erase, rewrite\n",
      "Erase again, and then\n",
      "A poppy blooms.\n",
      "\n",
      "Vectorized Texts:\n",
      "tf.Tensor(\n",
      "[[ 7  3  2  5]\n",
      " [ 2 10  9  4]\n",
      " [11  6  8  0]], shape=(3, 4), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 텍스트 데이터셋\n",
    "texts = [\"I write, erase, rewrite\", \"Erase again, and then\", \"A poppy blooms.\"]\n",
    "\n",
    "# 텍스트 변환\n",
    "vectorized_texts = text_vectorization(np.array(texts))\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Original Texts:\")\n",
    "for text in texts:\n",
    "    print(text)\n",
    "\n",
    "print(\"\\nVectorized Texts:\")\n",
    "print(vectorized_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ed5fd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f177f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16913434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6d46c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  10.3M      0  0:00:07  0:00:07 --:--:-- 10.8M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d6fe960",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a36976fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "949a032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e850fb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aff2a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'\"Cinema is the ultimate pervert art. It doesn\\'t give you what you desire; it tells you how to desire.\"<br /><br />So begins \"The Pervert\\'s Guide to Cinema,\" in which Slovenian philosopher and psychoanalyst Slavoj Zizek applies his Freudian/Lacanian brain-scalpel to world cinema. This film in three parts is the second feature documentary directed by Sophie Fiennes (yes, sister of Ralph and Joseph), and it is a notable accomplishment, clocking in at 2 1/2 hours of talk from one man and yet remaining humorous and engaging throughout. In essence, it is an extended film lecture, and one of the best you may ever get. Over the course of the film, Zizek guides us through a catalog of obsession and desire in film history. He touches on more than 40 films and, in particular, spends a great deal of time with Hitchcock, Lynch, Chaplin, Tarkovsky, the Marx Brothers, and Eisenstein. But he also takes a close look at \"Persona,\" \"The Conversation,\" \"Three Colors: Blue,\" \"Dogville,\" \"Fight Club,\" and \"The Exorcist.\" Thematically, Zizek\\'s inquiry into cinema ranges from thoughts on the death drive to the \"coordinates of desire,\" and from Gnosticism to \"partial objects.\"<br /><br />\"The Pervert\\'s Guide\" will be a slightly better experience if you\\'ve taken a few minutes to bone up on your basic Freudian terminology. However, even if you\\'re not steeped in psychoanalytic theory, Zizek\\'s dynamic and hilarious personality carries the film forward with such gusto that you aren\\'t likely to balk at the specialized lingo. The film frequently cuts from movie clips to images of Zizek *inside* the movie he is talking about--that is, in the original locations and sets. The transitions in these sequences sustain such tension and humor that the trick never gets old. And Zizek himself is constantly making us laugh, either from bizarre little jokes or from his enthusiastic insistence on, for example, a bold Oedipal interpretation of \"The Birds.\" And this go-ahead-and-laugh attitude, on the parts of both Fiennes and Zizek, is essential to the gonzo character of the film. It is the spoonful of sugar that helps us digest Zizek\\'s weird medicine. After all, don\\'t we all have a sense that, past a certain point, psychology theorists are just pulling our legs?', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d1ef218",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d277737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 0. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ee449e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af468dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 5s 6ms/step - loss: 0.4152 - accuracy: 0.8222 - val_loss: 0.2810 - val_accuracy: 0.8934\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2932 - accuracy: 0.8923 - val_loss: 0.2926 - val_accuracy: 0.8848\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2616 - accuracy: 0.9104 - val_loss: 0.2914 - val_accuracy: 0.8914\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2416 - accuracy: 0.9171 - val_loss: 0.2964 - val_accuracy: 0.8980\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2349 - accuracy: 0.9190 - val_loss: 0.3204 - val_accuracy: 0.8858\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2260 - accuracy: 0.9243 - val_loss: 0.3294 - val_accuracy: 0.8954\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2264 - accuracy: 0.9284 - val_loss: 0.3372 - val_accuracy: 0.8962\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2255 - accuracy: 0.9284 - val_loss: 0.3448 - val_accuracy: 0.8870\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2176 - accuracy: 0.9315 - val_loss: 0.3469 - val_accuracy: 0.8926\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2197 - accuracy: 0.9321 - val_loss: 0.3579 - val_accuracy: 0.8934\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2954 - accuracy: 0.8867\n",
      "테스트 정확도: 0.887\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.h5\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.h5\")\n",
    "print(f\"테스트 정확도: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18304f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3c5b7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3760 - accuracy: 0.8434 - val_loss: 0.2618 - val_accuracy: 0.9026\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2431 - accuracy: 0.9158 - val_loss: 0.2609 - val_accuracy: 0.9104\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2059 - accuracy: 0.9332 - val_loss: 0.2770 - val_accuracy: 0.9066\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1910 - accuracy: 0.9412 - val_loss: 0.2947 - val_accuracy: 0.9066\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1911 - accuracy: 0.9431 - val_loss: 0.3045 - val_accuracy: 0.9042\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1801 - accuracy: 0.9486 - val_loss: 0.3221 - val_accuracy: 0.9040\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1774 - accuracy: 0.9499 - val_loss: 0.3304 - val_accuracy: 0.9058\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1831 - accuracy: 0.9492 - val_loss: 0.3399 - val_accuracy: 0.9006\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1782 - accuracy: 0.9531 - val_loss: 0.3459 - val_accuracy: 0.9010\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1802 - accuracy: 0.9527 - val_loss: 0.3524 - val_accuracy: 0.8996\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2744 - accuracy: 0.9005\n",
      "테스트 정확도: 0.900\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.h5\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.h5\")\n",
    "print(f\"테스트 정확도: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23fa40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "943dff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d0484d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.4986 - accuracy: 0.7773 - val_loss: 0.2950 - val_accuracy: 0.8946\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.3334 - accuracy: 0.8630 - val_loss: 0.2859 - val_accuracy: 0.8934\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2993 - accuracy: 0.8788 - val_loss: 0.2906 - val_accuracy: 0.8968\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2674 - accuracy: 0.8895 - val_loss: 0.3060 - val_accuracy: 0.8862\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2559 - accuracy: 0.8925 - val_loss: 0.3111 - val_accuracy: 0.8844\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2365 - accuracy: 0.8995 - val_loss: 0.3170 - val_accuracy: 0.8838\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2371 - accuracy: 0.8972 - val_loss: 0.3245 - val_accuracy: 0.8916\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2290 - accuracy: 0.9011 - val_loss: 0.3488 - val_accuracy: 0.8646\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2191 - accuracy: 0.9004 - val_loss: 0.3451 - val_accuracy: 0.8856\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2237 - accuracy: 0.9017 - val_loss: 0.3468 - val_accuracy: 0.8800\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2972 - accuracy: 0.8894\n",
      "테스트 정확도: 0.889\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로 2.8.x 버전에서 TF-IDF 인코딩을 GPU에서 수행할 때 오류가 발생할 수 있습니다.\n",
    "# 텐서플로 2.9에서 이 이슈가 해결되었지만 코드를 테스트할 시점에 코랩의 텐서플로 버전은 2.8.2이기 때문에\n",
    "# 에러를 피하기 위해 CPU를 사용하여 텍스트를 변환합니다.\n",
    "with tf.device(\"cpu\"):\n",
    "    text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.h5\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.h5\")\n",
    "print(f\"테스트 정확도: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd7bf05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77c2f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 리뷰일 확률: 52.45 퍼센트\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"i cant see this\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"긍정적인 리뷰일 확률: {float(predictions[0] * 100):.2f} 퍼센트\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf0ed0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e41bf817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "tf.one_hot (TFOpLambda)      (None, None, 20000)       0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                5128448   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,128,513\n",
      "Trainable params: 5,128,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad43b319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 180s 252ms/step - loss: 0.5296 - accuracy: 0.7411 - val_loss: 0.3652 - val_accuracy: 0.8688\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 157s 251ms/step - loss: 0.3420 - accuracy: 0.8742 - val_loss: 0.2735 - val_accuracy: 0.8938\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 156s 250ms/step - loss: 0.2741 - accuracy: 0.9038 - val_loss: 0.2915 - val_accuracy: 0.8834\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 157s 251ms/step - loss: 0.2343 - accuracy: 0.9190 - val_loss: 0.3601 - val_accuracy: 0.8742\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 157s 251ms/step - loss: 0.2020 - accuracy: 0.9324 - val_loss: 0.2817 - val_accuracy: 0.8996\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 157s 251ms/step - loss: 0.1753 - accuracy: 0.9424 - val_loss: 0.3434 - val_accuracy: 0.8912\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 157s 251ms/step - loss: 0.1511 - accuracy: 0.9513 - val_loss: 0.3231 - val_accuracy: 0.8934\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 158s 252ms/step - loss: 0.1276 - accuracy: 0.9585 - val_loss: 0.3199 - val_accuracy: 0.8722\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 158s 252ms/step - loss: 0.1048 - accuracy: 0.9650 - val_loss: 0.3801 - val_accuracy: 0.8900\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 158s 252ms/step - loss: 0.0861 - accuracy: 0.9717 - val_loss: 0.4026 - val_accuracy: 0.8840\n",
      "782/782 [==============================] - 93s 118ms/step - loss: 0.3070 - accuracy: 0.8800\n",
      "테스트 정확도: 0.880\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.h5\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.h5\")\n",
    "print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16e98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
